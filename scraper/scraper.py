"""
–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å —Å–∫—Ä–∞–ø–µ—Ä–∞
"""

import logging
from typing import Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

from .config import BASE_URL, SECTIONS, DEFAULT_MAX_WORKERS, DEFAULT_DELAY
from .http_client import HTTPClient
from .parsers import HTMLParser
from .storage import DataStorage

logger = logging.getLogger(__name__)


class RBScraper:
    """–°–∫—Ä–∞–ø–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ rb.ru"""
    
    def __init__(self, max_workers: int = DEFAULT_MAX_WORKERS, delay: float = DEFAULT_DELAY):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞
        
        Args:
            max_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
        """
        self.http_client = HTTPClient(delay=delay)
        self.parser = HTMLParser()
        self.storage = DataStorage()
        self.max_workers = max_workers
        self.scraped_urls = set()
        self.articles = []
    
    def get_article_urls_from_listing(self, section: str, max_pages: int = 50) -> List[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ URL —Å—Ç–∞—Ç–µ–π –∏–∑ —Å–ø–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π —Ä–∞–∑–¥–µ–ª–∞
        
        Args:
            section: –ù–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ URL —Å—Ç–∞—Ç–µ–π
        """
        urls = []
        section_path = SECTIONS.get(section, '/')
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ä–∞–∑–¥–µ–ª–∞
        section_url = f"{BASE_URL}{section_path}" if section_path != '/' else BASE_URL
        soup = self.http_client.fetch_page(section_url)
        if soup:
            article_links = self.parser.extract_article_links(soup)
            urls.extend(article_links)
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(article_links)} —Å—Ç–∞—Ç–µ–π –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Ä–∞–∑–¥–µ–ª–∞ {section}")
        
        # –ó–∞—Ç–µ–º —Å–∫—Ä–∞–ø–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: https://rb.ru/news/?page=2
        for page in range(2, max_pages + 1):
            if section_path == '/':
                url = f"{BASE_URL}/?page={page}"
            else:
                url = f"{BASE_URL}{section_path}?page={page}"
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}: {url}")
            soup = self.http_client.fetch_page(url)
            if not soup:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}, –ø—Ä–µ–∫—Ä–∞—â–∞—é –ø–∞–≥–∏–Ω–∞—Ü–∏—é")
                break
            
            page_urls = self.parser.extract_article_links(soup)
            
            if not page_urls:
                logger.info(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} —Ä–∞–∑–¥–µ–ª–∞ {section}, –ø—Ä–µ–∫—Ä–∞—â–∞—é –ø–∞–≥–∏–Ω–∞—Ü–∏—é")
                break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ (–Ω–µ –¥—É–±–ª–∏–∫–∞—Ç—ã)
            new_urls = [u for u in page_urls if u not in urls]
            if not new_urls:
                logger.warning(f"–í—Å–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} —É–∂–µ –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã —Ä–∞–Ω–µ–µ, –ø—Ä–µ–∫—Ä–∞—â–∞—é –ø–∞–≥–∏–Ω–∞—Ü–∏—é")
                break
            
            urls.extend(new_urls)
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(new_urls)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} (–≤—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(set(urls))})")
        
        return list(set(urls))
    
    def parse_article_page(self, url: str) -> Dict:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏
        
        Args:
            url: URL —Å—Ç–∞—Ç—å–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ None
        """
        if url in self.scraped_urls:
            return None
        
        soup = self.http_client.fetch_page(url)
        if not soup:
            return None
        
        self.scraped_urls.add(url)
        return self.parser.parse_article(url, soup)
    
    def scrape_section(self, section: str, max_pages: int = 50, save_milestone: bool = False,
                       milestone_interval: int = None, total_before_section: int = 0) -> List[Dict]:
        """
        –°–∫—Ä–∞–ø–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–∞ —Å–∞–π—Ç–∞
        
        Args:
            section: –ù–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            save_milestone: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–∞
            milestone_interval: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ N —Å—Ç–∞—Ç–µ–π (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω)
            total_before_section: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–æ –Ω–∞—á–∞–ª–∞ —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞—é —Å–∫—Ä–∞–ø–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–∞: {section}")
        logger.info(f"[MILESTONE DEBUG] –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ—É–Ω–∫—Ü–∏–∏: milestone_interval={milestone_interval} (type: {type(milestone_interval)}), total_before_section={total_before_section}")
        urls = self.get_article_urls_from_listing(section, max_pages)
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –≤ —Ä–∞–∑–¥–µ–ª–µ {section}")
        if milestone_interval:
            logger.info(f"[MILESTONE] ‚úì Milestone —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∫–ª—é—á–µ–Ω–æ: –∫–∞–∂–¥—ã–µ {milestone_interval} —Å—Ç–∞—Ç–µ–π (—É–∂–µ —Å–æ–±—Ä–∞–Ω–æ: {total_before_section})")
        else:
            logger.warning(f"[MILESTONE] ‚úó Milestone —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –û–¢–ö–õ–Æ–ß–ï–ù–û! milestone_interval={milestone_interval}")
        
        articles = []
        last_saved_count = 0  # –ü–æ—Å–ª–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        
        logger.info(f"[MILESTONE] –ù–∞—á–∞–ª–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Ä–∞–∑–¥–µ–ª–∞ {section}: milestone_interval={milestone_interval}, total_before_section={total_before_section}")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_url = {executor.submit(self.parse_article_page, url): url for url in urls}
            
            with tqdm(total=len(urls), desc=f"–°–∫—Ä–∞–ø–∏–Ω–≥ {section}") as pbar:
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        article = future.result()
                        if article:
                            articles.append(article)
                            pbar.update(1)
                            
                            # –ü–†–û–°–¢–ê–Ø –ü–†–û–í–ï–†–ö–ê: –∫–∞–∂–¥—ã–µ N —Å—Ç–∞—Ç–µ–π
                            if milestone_interval and milestone_interval > 0:
                                total_articles = total_before_section + len(articles)
                                
                                # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 10 —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                                if len(articles) % 10 == 0:
                                    logger.info(f"[PROGRESS] –°—Ç–∞—Ç–µ–π –≤ —Ä–∞–∑–¥–µ–ª–µ: {len(articles)}, –≤—Å–µ–≥–æ: {total_articles}, last_saved: {last_saved_count}")
                                
                                # –£–ü–†–û–©–ï–ù–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É
                                if total_articles >= milestone_interval:
                                    # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ–ª—å–∫–æ —Å—Ç–∞—Ç–µ–π –ø—Ä–æ—à–ª–æ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                                    articles_since_last_save = total_articles - last_saved_count
                                    
                                    # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–∏ –∫ milestone
                                    if total_articles % 25 == 0 or articles_since_last_save >= milestone_interval - 5:
                                        logger.info(f"[CHECK] total={total_articles}, last_saved={last_saved_count}, —Ä–∞–∑–Ω–∏—Ü–∞={articles_since_last_save}, –Ω—É–∂–Ω–æ_—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å={articles_since_last_save >= milestone_interval}")
                                    
                                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ >= milestone_interval —Å—Ç–∞—Ç–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                                    if articles_since_last_save >= milestone_interval:
                                        logger.info(f"[MILESTONE] üî• –°–û–•–†–ê–ù–Ø–Æ: {total_articles} —Å—Ç–∞—Ç–µ–π (–±—ã–ª–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {last_saved_count}, —Ä–∞–∑–Ω–∏—Ü–∞: {articles_since_last_save})")
                                        
                                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
                                        temp_articles = list(self.articles) + articles
                                        logger.info(f"[MILESTONE] –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {len(temp_articles)} (–ø—Ä–µ–¥—ã–¥—É—â–∏–µ: {len(self.articles)}, —Ç–µ–∫—É—â–∏–µ: {len(articles)})")
                                        
                                        original_articles = self.articles
                                        self.articles = temp_articles
                                        
                                        try:
                                            logger.info(f"[MILESTONE] –ó–∞–ø–∏—Å—ã–≤–∞—é –≤ —Ñ–∞–π–ª—ã...")
                                            self.save_to_json('rb_articles_milestone.json')
                                            logger.info(f"[MILESTONE] JSON —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
                                            self.save_to_csv('rb_articles_milestone.csv')
                                            logger.info(f"[MILESTONE] CSV —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
                                            logger.info(f"[MILESTONE] ‚úÖ‚úÖ‚úÖ –£–°–ü–ï–®–ù–û –°–û–•–†–ê–ù–ï–ù–û {len(temp_articles)} –°–¢–ê–¢–ï–ô! ‚úÖ‚úÖ‚úÖ")
                                        except Exception as e:
                                            logger.error(f"[MILESTONE] ‚ùå –û–®–ò–ë–ö–ê: {e}", exc_info=True)
                                        finally:
                                            self.articles = original_articles
                                        
                                        # –û–±–Ω–æ–≤–ª—è–µ–º last_saved_count –Ω–∞ —Ç–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                                        last_saved_count = total_articles
                                        logger.info(f"[MILESTONE] –û–±–Ω–æ–≤–ª–µ–Ω last_saved_count = {last_saved_count}")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")
                        pbar.update(1)
            
            # –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ—Å–ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏ milestone
            if milestone_interval and milestone_interval > 0:
                total_articles = total_before_section + len(articles)
                if total_articles >= milestone_interval:
                    articles_since_last_save = total_articles - last_saved_count
                    if articles_since_last_save >= milestone_interval:
                        logger.info(f"[MILESTONE FINAL] üî• –§–ò–ù–ê–õ–¨–ù–û–ï –°–û–•–†–ê–ù–ï–ù–ò–ï: {total_articles} —Å—Ç–∞—Ç–µ–π (—Ä–∞–∑–Ω–∏—Ü–∞: {articles_since_last_save})")
                        temp_articles = list(self.articles) + articles
                        original_articles = self.articles
                        self.articles = temp_articles
                        try:
                            self.save_to_json('rb_articles_milestone.json')
                            self.save_to_csv('rb_articles_milestone.csv')
                            logger.info(f"[MILESTONE FINAL] ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(temp_articles)} —Å—Ç–∞—Ç–µ–π")
                        except Exception as e:
                            logger.error(f"[MILESTONE FINAL] ‚ùå –û–®–ò–ë–ö–ê: {e}", exc_info=True)
                        finally:
                            self.articles = original_articles
        
        logger.info(f"–°–∫—Ä–∞–ø–ª–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ {section}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ milestone –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–∞
        if save_milestone:
            # –í—Ä–µ–º–µ–Ω–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –¥–ª—è milestone
            temp_articles = list(self.articles) + articles
            json_file = f'rb_articles_milestone_{section}.json'
            csv_file = f'rb_articles_milestone_{section}.csv'
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            original_articles = self.articles
            self.articles = temp_articles
            self.save_to_json(json_file)
            self.save_to_csv(csv_file)
            self.articles = original_articles  # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
            logger.info(f"Milestone —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(temp_articles)} —Å—Ç–∞—Ç–µ–π –≤ {json_file} –∏ {csv_file}")
        
        return articles
    
    def scrape_all(self, max_pages_per_section: int = 20, pages_config: dict = None, 
                   save_milestones: bool = True, milestone_interval: int = None) -> List[Dict]:
        """
        –°–∫—Ä–∞–ø–∏–Ω–≥ –≤—Å–µ—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ —Å–∞–π—Ç–∞
        
        Args:
            max_pages_per_section: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ —Ä–∞–∑–¥–µ–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            pages_config: –°–ª–æ–≤–∞—Ä—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
                         –ù–∞–ø—Ä–∏–º–µ—Ä: {'news': 200, 'stories': 200, 'opinions': 50}
                         –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è max_pages_per_section –¥–ª—è –≤—Å–µ—Ö
            save_milestones: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
            milestone_interval: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ N —Å—Ç–∞—Ç–µ–π (–µ—Å–ª–∏ None, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
        """
        all_articles = []
        self.articles = []  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è
        
        logger.info(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: save_milestones={save_milestones}, milestone_interval={milestone_interval}")
        
        for section in SECTIONS.keys():
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ pages_config –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                max_pages = pages_config.get(section, max_pages_per_section) if pages_config else max_pages_per_section
                logger.info(f"–°–∫—Ä–∞–ø–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–∞ {section} —Å –º–∞–∫—Å–∏–º—É–º–æ–º {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
                
                # –ü–µ—Ä–µ–¥–∞–µ–º milestone_interval –∏ —Ç–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –≤ scrape_section
                # –í–ê–ñ–ù–û: –ø–µ—Ä–µ–¥–∞–µ–º –≤—Å–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ self.articles –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                self.articles = all_articles  # –û–±–Ω–æ–≤–ª—è–µ–º –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–∞–∑–¥–µ–ª–∞
                logger.info(f"–ü–µ—Ä–µ–¥–∞—é –≤ scrape_section: milestone_interval={milestone_interval}, total_before_section={len(all_articles)}")
                articles = self.scrape_section(
                    section, 
                    max_pages, 
                    save_milestone=save_milestones,
                    milestone_interval=milestone_interval,
                    total_before_section=len(all_articles)
                )
                all_articles.extend(articles)
                self.articles = all_articles  # –û–±–Ω–æ–≤–ª—è–µ–º –¥–ª—è milestone —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                
                logger.info(f"–í—Å–µ–≥–æ —Å–∫—Ä–∞–ø–ª–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(all_articles)}")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–µ —Ä–∞–∑–¥–µ–ª–∞ {section}: {e}")
        
        self.articles = all_articles
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª—É –ø–µ—Ä–µ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ–º
        if milestone_interval and len(all_articles) >= milestone_interval:
            self.save_to_json('rb_articles_milestone.json')
            self.save_to_csv('rb_articles_milestone.csv')
            logger.info(f"–§–∏–Ω–∞–ª—å–Ω—ã–π milestone —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(all_articles)} —Å—Ç–∞—Ç–µ–π")
        
        return all_articles
    
    def save_to_json(self, filename: str = 'rb_articles.json'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON"""
        self.storage.save_to_json(self.articles, filename)
    
    def save_to_csv(self, filename: str = 'rb_articles.csv'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ CSV"""
        self.storage.save_to_csv(self.articles, filename)

