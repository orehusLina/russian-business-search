# Решение проблем

## Исправленные проблемы

### 1. ❌ "Нет данных для сохранения"

**Проблема:** В `test_small.py` статьи не сохранялись в `self.articles` перед вызовом `save_to_json()`.

**Решение:** Добавлено `scraper.articles = articles` перед сохранением.

### 2. ❌ Ошибки соединения (Connection aborted, RemoteDisconnected)

**Проблема:** Сайт закрывает соединения при слишком частых запросах.

**Решения:**
- Изменен `Connection: keep-alive` на `Connection: close`
- Увеличено количество попыток с 3 до 5
- Увеличен timeout с 10 до 15 секунд
- Добавлена специальная обработка ошибок соединения
- Увеличена задержка между повторами (до 30 секунд максимум)

### 3. ⚠️ Пустые поля (компании, инвестиции)

**Возможные причины:**
- Текст статьи не извлекается полностью
- Регулярные выражения не находят совпадения
- Структура сайта изменилась

**Что делать:**
- Проверьте, что поле `text` заполнено
- Улучшите регулярные выражения в `extractors.py`
- Проверьте структуру HTML страницы

## Рекомендации

### Если ошибки соединения продолжаются:

1. **Увеличьте задержку:**
```python
scraper = RBScraper(max_workers=2, delay=2.0)  # Меньше потоков, больше задержка
```

2. **Уменьшите количество потоков:**
```python
scraper = RBScraper(max_workers=1, delay=2.0)  # Один поток
```

3. **Используйте VPN или прокси** (если сайт блокирует ваш IP)

### Если данные не извлекаются:

1. Проверьте логи в `scraper.log`
2. Сохраните HTML страницы для анализа:
```python
# В parsers.py добавьте:
with open(f'debug_{url.split("/")[-1]}.html', 'w', encoding='utf-8') as f:
    f.write(soup.prettify())
```

3. Проверьте, что селекторы в `parsers.py` соответствуют структуре сайта

## Логи

Все ошибки логируются в `scraper.log`. Проверьте его для диагностики проблем.

